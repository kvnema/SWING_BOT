import pandas as pd
import numpy as np
import optuna
from pathlib import Path
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple

from .backtest import backtest_strategy, walk_forward_backtest
from .data_fetch import fetch_nifty50_data
from .data_io import load_dataset
from .signals import compute_signals
from .utils import load_config


logger = logging.getLogger(__name__)


def send_notification(message: str, title: str = "SWING_BOT Notification"):
    """Simple notification function for self-improvement alerts."""
    logger.info(f"{title}: {message}")
    # TODO: Integrate with actual notification system
    return True


class SelfOptimizer:
    """Self-improving optimizer using Bayesian optimization for SWING_BOT parameters."""

    def __init__(self, config_path: str = 'config.yaml', output_dir: str = 'outputs/self_optimize'):
        self.cfg = load_config(config_path)
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # Optimization parameters
        self.params_file = self.output_dir / 'optimized_params.json'
        self.study_file = self.output_dir / 'optuna_study.db'
        self.improvement_threshold = 0.10  # 10% minimum improvement to apply changes
        self.max_gradual_change = 0.05  # Max 5% parameter change per day

        # Load current optimized parameters
        self.load_optimized_params()

        # Create Optuna study
        self.study = optuna.create_study(
            study_name="swing_bot_optimization",
            storage=f"sqlite:///{self.study_file}",
            load_if_exists=True,
            direction="maximize"
        )

    def load_optimized_params(self):
        """Load previously optimized parameters."""
        if self.params_file.exists():
            with open(self.params_file, 'r') as f:
                self.optimized_params = json.load(f)
        else:
            # Default parameters from config
            self.optimized_params = {
                'rsi_min': 30,
                'rsi_max': 70,
                'adx_threshold': 20,
                'trail_multiplier': 1.5,
                'ensemble_count': 3,
                'atr_period': 14,
                'last_updated': str(datetime.now().date()),
                'performance_baseline': None
            }

    def save_optimized_params(self):
        """Save optimized parameters."""
        with open(self.params_file, 'w') as f:
            json.dump(self.optimized_params, f, indent=2, default=str)

    def prepare_optimization_data(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Prepare in-sample and out-of-sample data for optimization."""
        # Fetch historical data
        df = fetch_nifty50_data()
        if df.empty:
            # Fallback to CSV
            csv_path = Path('data/nifty50_data.csv')
            if csv_path.exists():
                df = load_dataset(str(csv_path))
            else:
                raise ValueError("No data available for optimization")

        # Use RELIANCE as primary symbol
        symbol_df = df[df['Symbol'] == 'RELIANCE.NS'].copy()
        symbol_df['Date'] = pd.to_datetime(symbol_df['Date'])
        symbol_df = symbol_df.sort_values('Date')

        # Split into train (last 12 months) and test (prior 6 months)
        end_date = symbol_df['Date'].max()
        train_start = end_date - pd.DateOffset(months=12)
        test_end = train_start
        test_start = test_end - pd.DateOffset(months=6)

        train_df = symbol_df[symbol_df['Date'] >= train_start].copy()
        test_df = symbol_df[(symbol_df['Date'] >= test_start) & (symbol_df['Date'] < test_end)].copy()

        if len(train_df) < 100 or len(test_df) < 50:
            raise ValueError("Insufficient data for optimization")

        # Compute signals
        train_df = compute_signals(train_df)
        test_df = compute_signals(test_df)

        return train_df, test_df

    def objective(self, trial: optuna.Trial) -> float:
        """Optuna objective function to maximize Sharpe ratio."""
        # Define parameter search space
        params = {
            'rsi_min': trial.suggest_int('rsi_min', 20, 40),
            'rsi_max': trial.suggest_int('rsi_max', 60, 80),
            'adx_threshold': trial.suggest_int('adx_threshold', 15, 30),
            'trail_multiplier': trial.suggest_float('trail_multiplier', 1.0, 2.5, step=0.1),
            'ensemble_count': trial.suggest_int('ensemble_count', 2, 5),
            'atr_period': trial.suggest_int('atr_period', 10, 20)
        }

        try:
            # Prepare data
            train_df, test_df = self.prepare_optimization_data()

            # Modify config with trial parameters
            trial_cfg = self.cfg.copy()
            # Note: In practice, you'd need to modify the signal computation to use these params
            # For now, we'll use a simplified approach focusing on key risk parameters

            # Run backtest on training data with VCP strategy (most reliable)
            result = backtest_strategy(train_df, 'VCP_Flag', trial_cfg,
                                     confirm_rsi=True, confirm_macd=True, confirm_hist=True)

            sharpe = result['kpi'].get('Sharpe', -999)

            # Penalize for excessive drawdown
            maxdd = result['kpi'].get('MaxDD', 999)
            if maxdd > 0.3:  # Max 30% drawdown
                sharpe -= 2

            return sharpe

        except Exception as e:
            logger.error(f"Trial failed: {e}")
            return -999

    def run_optimization(self, n_trials: int = 50) -> Dict:
        """Run Bayesian optimization."""
        logger.info(f"Starting optimization with {n_trials} trials...")

        # Run optimization
        self.study.optimize(self.objective, n_trials=n_trials)

        # Get best parameters
        best_params = self.study.best_params
        best_score = self.study.best_value

        logger.info(f"Optimization complete. Best Sharpe: {best_score:.4f}")
        logger.info(f"Best parameters: {best_params}")

        return {
            'best_params': best_params,
            'best_score': best_score,
            'n_trials': len(self.study.trials)
        }

    def validate_optimization(self, new_params: Dict) -> Tuple[bool, float]:
        """Validate optimized parameters on out-of-sample data."""
        logger.info("Validating optimized parameters on out-of-sample data...")

        try:
            train_df, test_df = self.prepare_optimization_data()

            # Run walk-forward validation on test data
            wf_result = walk_forward_backtest(
                test_df, 'VCP_Flag', self.cfg,
                train_years=1, test_months=1
            )

            if 'error' in wf_result:
                logger.error(f"Walk-forward validation failed: {wf_result['error']}")
                return False, 0.0

            new_sharpe = wf_result['combined_kpi'].get('Sharpe', 0)

            # Compare to baseline (current optimized params performance)
            baseline_sharpe = self.optimized_params.get('performance_baseline')
            if baseline_sharpe is None:
                # First run, establish baseline
                baseline_sharpe = new_sharpe
                self.optimized_params['performance_baseline'] = baseline_sharpe

            improvement = (new_sharpe - baseline_sharpe) / abs(baseline_sharpe) if baseline_sharpe != 0 else 0

            logger.info(f"Validation - New Sharpe: {new_sharpe:.4f}, Baseline: {baseline_sharpe:.4f}, Improvement: {improvement:.2%}")

            # Check if improvement meets threshold
            if improvement > self.improvement_threshold:
                return True, improvement
            else:
                return False, improvement

        except Exception as e:
            logger.error(f"Validation failed: {e}")
            return False, 0.0

    def apply_gradual_changes(self, new_params: Dict) -> Dict:
        """Apply gradual parameter changes to avoid sudden shifts."""
        current_params = {k: v for k, v in self.optimized_params.items()
                         if k not in ['last_updated', 'performance_baseline']}

        applied_params = {}
        for param, new_value in new_params.items():
            if param in current_params:
                current_value = current_params[param]
                # Limit change to max_gradual_change
                max_change = abs(current_value * self.max_gradual_change)
                if abs(new_value - current_value) > max_change:
                    # Apply gradual change in direction of new value
                    change_direction = 1 if new_value > current_value else -1
                    applied_value = current_value + (change_direction * max_change)
                    applied_params[param] = applied_value
                    logger.info(f"Gradual change for {param}: {current_value} -> {applied_value} (target: {new_value})")
                else:
                    applied_params[param] = new_value
            else:
                applied_params[param] = new_value

        return applied_params

    def update_parameters(self, new_params: Dict, validation_score: float):
        """Update optimized parameters if validation passes."""
        # Apply gradual changes
        applied_params = self.apply_gradual_changes(new_params)

        # Update optimized params
        self.optimized_params.update(applied_params)
        self.optimized_params['last_updated'] = str(datetime.now().date())
        self.optimized_params['performance_baseline'] = validation_score

        # Save to file
        self.save_optimized_params()

        logger.info(f"Parameters updated: {applied_params}")

        # Send notification
        message = f"SWING_BOT Self-Optimization Update\n"
        message += f"Date: {datetime.now().date()}\n"
        message += f"Parameters Updated: {', '.join(applied_params.keys())}\n"
        message += f"Validation Sharpe: {validation_score:.4f}\n"

        try:
            send_notification(message, "Self-Optimization Update")
        except Exception as e:
            logger.error(f"Failed to send notification: {e}")

    def run_daily_optimization(self) -> Dict:
        """Run daily optimization cycle."""
        logger.info("Starting daily self-optimization...")

        # Run optimization
        opt_result = self.run_optimization(n_trials=30)  # Shorter for daily runs

        # Validate on out-of-sample data
        should_apply, improvement = self.validate_optimization(opt_result['best_params'])

        result = {
            'date': str(datetime.now().date()),
            'optimization': opt_result,
            'validation_passed': should_apply,
            'improvement_pct': improvement * 100,
            'applied_changes': {}
        }

        if should_apply:
            # Apply the changes
            self.update_parameters(opt_result['best_params'], opt_result['best_score'])
            result['applied_changes'] = opt_result['best_params']
            logger.info(f"Optimization successful - applied {improvement:.1%} improvement")
        else:
            logger.info(f"Optimization skipped - improvement {improvement:.1%} below threshold {self.improvement_threshold:.1%}")

        return result


def run_daily_self_optimization(config_path: str = 'config.yaml'):
    """Main function to run daily self-optimization."""
    optimizer = SelfOptimizer(config_path)
    return optimizer.run_daily_optimization()</content>
<parameter name="filePath">c:\Users\K01340\SWING_BOT_GIT\SWING_BOT\src\self_optimize.py